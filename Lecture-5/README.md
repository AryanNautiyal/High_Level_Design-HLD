


*** Rate Limiting ***

    -- So a client can send the server any number of requests which can overload the server

    # Problems :

        -- Client can send too many requests within a time frame 

        -- DOS/DDOS attacks can happen 

        -- Server cost expensive like google maps charge for each request if we are using it for our business so if a client sends
            too many requests at once then the cost will be more 

    
    -- All these problems are solved by the rate limiter 

    -- So rate limiting helps in limiting the number of requests client can send to a server

    -- So now question is where should we implement rate limiter : 1. Client side
                                                                   2. Server side
                                                                   3. Middleware

    -- So client side is the worst as at client side we do not have control and client can change the code and client is easier to 
        get hacked

    -- Server side and middleware are both good to implement rate limiter

    -- So middleware is in between client and the server so client sends request to middleware then middleware sends to server 
        similarly server sends response to middleware and middleware sends it back to the client 

    -- API Gateway works as a middleware in which we can implement many logics out of which one is rate limiter 

    -- Rate limiting is just used to validate how many request user can send at a particular time frame

    -- So we have like server side languages like Java, C++, JS, Python so if our server side language is fast then we can 
        implement rate limiting in server side only

    -- Generally 3rd party rate limiter providers are there so we can implement them in our middleware also (API Gateway)

    -- We won't be needed to implement rate limiter and no need to manage them if we get it from rate limiter service providers

    -- So rate limiting can be done on : 1. IP address basis
                                         2. UserId basis
                                         3. Other way

    -- So each request sent by client will have same IP address so we can apply rate limiting by IP as like limit the number of 
        requests sent by this IP address

    -- (IP not much better as discussed in LB lecture that in a company all can have same IP for security purposes so it will limit
        requests sent by many people from same IP)

    -- If rate limiter is implemented in server side so there are two ways by which we reject the request sent by the user :

                1. Dropping down the request : In this the request is just drop without saying anything to the client

                2. Sending a response back to the client : In this a status code of 429 is sent from server to client to tell client
                                                            that it has hit the limit 

    -- Status Code 429 : Too many requests 

    -- While sending response back to client with status code 429 we can also tell the client in Headers like X-api-limit : 50,
        X-duration : seconds 

    -- So this way we can inform client how many request it can send in a sec 

    -- Headers are also sent with response code also 

    -- So the 3rd way to handle status code 429 is that we don't send any response to user and we also don't drop the request sent 
        by the user instead we know that we can handle this request later as for now user has sent too many requests so we put this
        request in a message queue to execute it later (nowadays this is used)

    -- So in message queue the request is served after sometime 

    -- So we should implement intelligent system so that if request is important then it is put in message queue and if request is 
        not important then we return status code 429

    ## Algorithms of Rate Limiting ##

        1. Token Bucket
        2. Leaky Bucket
        3. Fixed Window Counter
        4. Sliding Window Log
        5. Sliding Window Counter

        -- Taking assumption here that all our rate limiting algorithms are implemented in our middleware


        1. Token Bucket Algorithm

                -- So in this there are request pool which consists of all the requests, bucket which consists of tokens and token 
                    builder which builder token to store in the bucket and server which serves the request sent by client

                -- So token builder has a speed or can say inflow like 2 token/sec so token builder builds 2 token per second 

                -- So these tokens generated by token builder are kept in bucket and bucket has a maximum capacity 

                -- So if the bucket is full and token builder sends more token to be stored then the tokens are overflowed 

                -- So the requests comes from request pool and take a token and go to server to get the request served

                -- If a request comes and there's no token left then the request is either dropped or response status code 429 is 
                    sent

                -- So it's kind of like a line in which each person gets a token to enter the shop and those without token cannot 
                    enter the shop 

                -- Token builder rate should be decided based on our application and bucket size also 

                -- Used by Amazon 

                # Pros

                    -- Simple to implement

                    -- Doesn't use much memory 

                    -- Can handle burst traffic for small duration like in end of year sale a user can send many requests at once 
                        so therefore it can handle burst traffic as we need to take bucket size and token builder according to the 
                        traffic 

                # Cons 

                    -- To decide the bucket capacity and inflow rate we need to have knowledge about our server 

        
        2. Leaky Bucket Algorithm

                -- Similar to token bucket algorithm just slightly different from it 

                -- In this instead of a token builder and a bucket that contains token built by token builder there is a bucket
                    (queue) which consists of requests from request pool and there is a fixed outflow which is how many req/sec
                    will exit bucket and go to server

                -- So in this we do not want server to handle burst traffic so we have fixed an interval so server will handle 
                    specific amount of requests per interval

                -- So requests comes from request pool and they are placed in the bucket 

                -- Let's say outflow is 2 req/sec so server will handle 2 request from bucket per second 

                -- So every second 2 request will be taken out of the bucket and sent to the server

                -- So bucket will have a max capacity 

                -- So limited number of requests can stay in the bucket and if 2 request are sent to server then 2 request can be 
                    stored in queue as when 2 req goes to server 2 spaces are left empty for new request to occupy

                -- After the bucket is full all the new request is dropped or response code 429 is sent 

                -- So outflow and bucket size is fixed here and due to outflow there is no need for server to handle burst traffic 

                -- Used by Shopify

                # Pros

                    -- Simple to implement 

                    -- Even though burst traffic is there our server will not crash as outflow is fixed

                # Cons

                    -- DDoS/DoS attacks will send many requests and our queue will be full by the unnecessary requests and the 
                        necessary requests won't be handled (or can say our necessary request will starve)

        
        ## Cons of Both Leaky Token Bucket and Token Bucket ##

            -- So if we apply these algorithms globally then it will be not good as billions of request will come to server and rate
                limiter will only serve 5 request let's say so it's bad
                So if we take based on userId then for each userId we will be needed to create a bucket 
                So if we take IP then for each IP also we will be needed to create a bucket for each IP
                The DoS/DDoS won't affect that much to Token Bucket Algorithm as compared to Leaky Token Bucket algorithm as in 
                token bucket algo the server can handle burst traffic based on number of tokens in the bucket so much more chances
                of necessary request to go to server

            -- Application has 100 APIs so sometimes APIs wise rate limiting is also necessary as if we use google maps api then we 
                will be needed to implement a rate limiter for it as it can increase cost 
                So for 100 APIs different different rate limiters will be made and each will have it's own bucket so in here instead
                of making it for each user we can make it for each API that is the logic here


    3. Fixed Window Counter

        -- It's just fixed window rate limiter

        -- In this a window is fixed or intervals are fixed in which the server handles a specific number of requests 

        -- So let's say we have window size or fixed counter as 3 and interval is in minutes so server will handle 3 req/min

        -- Every min the limit refreshes and again 3 requests can be sent to the server 

        -- More like if we say that for every 60 mins max number of request sent by a user can be 100 so 100 req/hr will be handled
            by the server

        # Cons

            -- If burst traffic comes at edges of window it may lead to high latency or server crash like let's saw we have window 
                size of 9-10 pm and in each window 100 request can be sent so at 9:59 user sends all 100 API calls and then at 
                10:01 also it sends 100 API requests as window resets every hour as window size is 60 mins so 200 APIs request is 
                sent to server at once which leads to latency and can crash server as instantly more load is put on server 

                To overcome this problem we use Sliding Window Log Algorithm 

    
    4. Sliding Window Log Algorithm 

        -- It is the best algorithm for rate limiting 

        -- Drawback of it is that it is slow and memory consuming 

        -- It is very strict 

        # Algorithm :

                -- It will store each request in a log file

                -- Whenever a request comes, it will first remove all the outdated request from the file

                -- Then it will log the new request and check if the counter limit reached drop the request else send it to the 
                    server 
            
        -- We generally store epoch time here (can see this by epoch converter)

        -- In this we take a window size and this window slides 

        -- So each request is logged in a log file with timestamp 

        -- Let's say our server can handle 3 req/min so if r1 comes at 00:01 then so at 1 sec the request 1 comes then r2 comes at 
            00:30 so at 30 seconds r2 comes then at 00:59 so at 59 seconds r3 comes so 3 request is in the log files as per minute
            requests are logged in log files then when r4 comes at 01:02 meaning 1 min 2 seconds then it checks log file for the 
            outdated request (it checks for outdated request in log files every time a new request comes) in the log file so as
            r1 is outdated so it's removed from the log file and then r4 is sent to server and it's timestamp is noted in the log 
            file

        -- So if r5 comes at 01:04 meaning 1 min 4 sec then it checks log file for outdated requests but as there are no outdated 
            request so then it checks the limit as 3 req/min is the limit so therefore r5 is either dropped or respones code 429
            is sent to the user

        # Cons

            -- Time consuming (each time request comes it checks for outdated request and then logs it into the log file)

            -- Memory consuming (as all the requests will be stored in the log file for a particular time until they are outdated)

        # Pros

            -- Very strict

            -- Allows specific number of requests to be served for a particular window 

            -- Overcomes problem occurred in fixed window counter algorithm

        
    5. Sliding Window Counter Algorithm

        -- It is a hybrid approach of sliding window log algorithm and fixed window counter algorithm 

        -- In this a formula is used 

            Formula : Number of requests in current window + Number of requests in previous window * Overlap percentage

            Logic : "How much of the previous minute's traffic is still 'relevant' to our current 60-second sliding window?"

        -- So if our server has like 7 req/min then when let's assume r1 comes at 00:30 then r2, r3, r4, r5 comes at 00:58 then 
            after r6, r7, r8 comes at 1:05 then it calculates using formula let's assume 70% request overlap in previous window 
            with current window having 30% overlap

                So 3 + 5 * 70% = 3 + 5 * 0.7 = 3 + 3.5 = 6.5

                So as 6.5 < 7 so hence new request will be logged and sent to server

                So it will send some request more that limit only but not too much to crash the server

                So if we assume that r9 comes then

                So 3 + 6 * 0.7 = 3 + 4.2 = 7.2

                So as 7.2 < 7 therefore r9 will be dropped or response code 429 will be sent to user

        -- Hence it works like that but overlap should be calculated


*** Creating our own Rate Limiter ***

    -- Which Algorithm to implement ? (Using fixed window counter algo here)

    -- Expected counter and current counter 

            Expected is the limit till which it accepts the requests whereas the current is the current number of requests that are
            there

    -- Where to store these counter values ?

            Cannot store in DB as it's slow and we don't want our rate limiter to work slow 

            So we will store it in Cache and best is in-memory cache like Redis

    -- Where will we implement our rate limiting ?

            We can implement it in middleware or server side also


    So first we will implement Redis and with it we will set expiry of each value stored in it so that after window size is done
    all the outdated records will be removed and we can count all the valid requests that are currently there

    So Rate Limiter will talk to redis but we also need to give rate limiter rules also meaning the algorithm it will follow

    So we will store these rules in DB as Redis in in-memory cache which is volatile but we can see that DB is slow so we will 
    introduce cache here to speed up the rules fetching each time request comes to rate limiter

    So we will introduce workers for DB as if we change rules in DB then also rate limiter will fetch old rules through cache only
    like if we want to increase the count like 7 to 10 req/min so we need workers there

    What workers do is that they just take data from DB and give it to cache from time to time so that data in cache is updated so 
    that even if we change the rules then also the data in cache will be updated

    So now our rate limiter is independent of the rules so loosely coupled now

    We can either drop the request or send response code 429 : Too many requests or can put request in messaging queue

    Messaging queue : Kafka or RabbitMQ

    So we will also implement workers between messaging queue and server so that after when server is not busy then workers will 
    give request from queue to server

    
        